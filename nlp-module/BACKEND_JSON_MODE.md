# ðŸ”§ Backend NLP - Mode JSON (Accepter donnÃ©es d'Å“uvres)

Le frontend envoie maintenant les donnÃ©es complÃ¨tes des Å“uvres depuis le fichier JSON local au lieu d'utiliser des IDs de base de donnÃ©es.

---

## ðŸŽ¯ Changements Requis dans le Backend

### ProblÃ¨me Actuel

```
Frontend â†’ POST /conversation/start?artwork_id=1
Backend â†’ 422 Error (artwork_id=1 n'existe pas en DB)
```

### Solution

```
Frontend â†’ POST /conversation/start
Body: { "artwork_data": { titre, artiste, description, ... } }

Backend â†’ Accepte les donnÃ©es directement
Backend â†’ CrÃ©e session avec contexte artwork
Backend â†’ Retourne message de bienvenue
```

---

## ðŸ“ Modifications Backend NÃ©cessaires

### 1. Endpoint `/conversation/start`

**Fichier:** `nlp-module/backend-app/app/main.py` (ou routes conversation)

**Avant:**
```python
@app.post("/conversation/start")
async def start_conversation(artwork_id: str):
    # Cherche artwork dans DB par ID
    artwork = await db.get_artwork(artwork_id)
    if not artwork:
        raise HTTPException(404, "Artwork not found")
    # ...
```

**AprÃ¨s:**
```python
from pydantic import BaseModel

class ArtworkData(BaseModel):
    id: str
    code: str
    title: str
    artist: str
    description: str
    period: str
    style: str
    collection: str
    country: str

class StartConversationRequest(BaseModel):
    artwork_data: ArtworkData

@app.post("/conversation/start")
async def start_conversation(request: StartConversationRequest):
    # Utilise directement les donnÃ©es reÃ§ues
    artwork = request.artwork_data

    # CrÃ©e session
    session_id = str(uuid.uuid4())

    # GÃ©nÃ¨re message de bienvenue avec LLM
    welcome_message = await generate_welcome_message(artwork)

    # GÃ©nÃ¨re audio TTS
    audio_url = await generate_tts(welcome_message)

    # Stocke session en mÃ©moire
    sessions[session_id] = {
        "artwork": artwork.dict(),
        "history": []
    }

    return {
        "session_id": session_id,
        "message": welcome_message,
        "audio_url": audio_url
    }
```

---

### 2. Endpoint `/conversation/text`

**Avant:**
```python
@app.post("/conversation/text")
async def send_text_message(session_id: str, message: str):
    session = sessions.get(session_id)
    if not session:
        raise HTTPException(404, "Session not found")

    # GÃ©nÃ¨re rÃ©ponse...
```

**AprÃ¨s:**
```python
class TextMessageRequest(BaseModel):
    session_id: str
    message: str
    artwork_data: ArtworkData  # Contexte de l'Å“uvre

@app.post("/conversation/text")
async def send_text_message(request: TextMessageRequest):
    session_id = request.session_id
    message = request.message
    artwork = request.artwork_data

    # RÃ©cupÃ¨re ou crÃ©e session
    if session_id not in sessions:
        sessions[session_id] = {
            "artwork": artwork.dict(),
            "history": []
        }

    session = sessions[session_id]

    # Ajoute message utilisateur Ã  l'historique
    session["history"].append({"role": "user", "content": message})

    # GÃ©nÃ¨re rÃ©ponse IA avec contexte
    response = await generate_ai_response(
        message=message,
        artwork=artwork.dict(),
        history=session["history"]
    )

    # GÃ©nÃ¨re audio
    audio_url = await generate_tts(response)

    # Ajoute rÃ©ponse Ã  l'historique
    session["history"].append({"role": "assistant", "content": response})

    return {
        "response": response,
        "audio_url": audio_url,
        "intent": "general_question"
    }
```

---

### 3. Fonction `generate_welcome_message()`

```python
async def generate_welcome_message(artwork: ArtworkData) -> str:
    """GÃ©nÃ¨re message de bienvenue en franÃ§ais"""

    prompt = f"""Tu es Musia, un guide musÃ©al AI qui parle franÃ§ais.

Å’uvre d'art sÃ©lectionnÃ©e :
- Titre : {artwork.title}
- Artiste : {artwork.artist}
- Description : {artwork.description}
- PÃ©riode : {artwork.period}
- Style : {artwork.style}
- Collection : {artwork.collection}
- Pays : {artwork.country}

GÃ©nÃ¨re un message de bienvenue chaleureux en franÃ§ais (2-3 phrases) qui :
1. Accueille le visiteur
2. PrÃ©sente briÃ¨vement l'Å“uvre
3. Invite Ã  poser des questions

RÃ©ponds UNIQUEMENT le message de bienvenue, sans explications."""

    # Appel Ã  Groq LLM
    response = await call_groq_llm(prompt)

    return response
```

---

### 4. Fonction `generate_ai_response()`

```python
async def generate_ai_response(
    message: str,
    artwork: dict,
    history: list
) -> str:
    """GÃ©nÃ¨re rÃ©ponse IA contextuelle en franÃ§ais"""

    # Construit le contexte
    system_prompt = f"""Tu es Musia, un expert en art africain qui parle franÃ§ais.

Å’uvre actuellement discutÃ©e :
- Titre : {artwork['title']}
- Artiste : {artwork['artist']}
- Description : {artwork['description']}
- PÃ©riode : {artwork['period']}
- Style : {artwork['style']}
- Collection : {artwork['collection']}
- Pays : {artwork['country']}

Instructions :
1. RÃ©ponds TOUJOURS en franÃ§ais
2. Sois prÃ©cis et informatif
3. Utilise le contexte de l'Å“uvre fournie
4. Reste concentrÃ© sur l'art africain
5. Si la question est hors sujet, ramÃ¨ne poliment vers l'Å“uvre

RÃ©ponds Ã  la question de maniÃ¨re dÃ©taillÃ©e et engageante."""

    # Construit l'historique pour le LLM
    messages = [
        {"role": "system", "content": system_prompt}
    ]

    # Ajoute l'historique rÃ©cent (derniers 5 messages)
    for msg in history[-5:]:
        messages.append(msg)

    # Appel Ã  Groq
    response = await call_groq_llm_with_history(messages)

    return response
```

---

### 5. Fonction `call_groq_llm()`

```python
import os
from groq import AsyncGroq

client = AsyncGroq(api_key=os.getenv("GROQ_API_KEY"))

async def call_groq_llm(prompt: str) -> str:
    """Appel simple Ã  Groq"""
    response = await client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=500
    )

    return response.choices[0].message.content

async def call_groq_llm_with_history(messages: list) -> str:
    """Appel avec historique"""
    response = await client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=messages,
        temperature=0.7,
        max_tokens=500
    )

    return response.choices[0].message.content
```

---

### 6. Gestion des Sessions en MÃ©moire

```python
from typing import Dict
import uuid

# Stockage en mÃ©moire (reset au redÃ©marrage)
sessions: Dict[str, dict] = {}

def create_session(artwork_data: dict) -> str:
    """CrÃ©e une nouvelle session"""
    session_id = str(uuid.uuid4())
    sessions[session_id] = {
        "artwork": artwork_data,
        "history": [],
        "created_at": datetime.now()
    }
    return session_id

def get_session(session_id: str) -> dict:
    """RÃ©cupÃ¨re une session"""
    return sessions.get(session_id)

def cleanup_old_sessions():
    """Nettoie les sessions de plus de 24h"""
    now = datetime.now()
    expired = [
        sid for sid, data in sessions.items()
        if (now - data["created_at"]).hours > 24
    ]
    for sid in expired:
        del sessions[sid]
```

---

## ðŸ”Š GÃ©nÃ©ration Audio TTS

```python
import edge_tts
import hashlib
import os

async def generate_tts(text: str) -> str:
    """GÃ©nÃ¨re audio MP3 avec Edge TTS"""

    # Hash pour cache
    text_hash = hashlib.md5(text.encode()).hexdigest()
    cache_dir = "data/tts_cache"
    os.makedirs(cache_dir, exist_ok=True)
    audio_file = f"{cache_dir}/{text_hash}.mp3"

    # Si dÃ©jÃ  en cache
    if os.path.exists(audio_file):
        return f"/audio/{text_hash}.mp3"

    # GÃ©nÃ¨re avec Edge TTS (voix franÃ§aise)
    communicate = edge_tts.Communicate(
        text=text,
        voice="fr-FR-DeniseNeural"  # Voix fÃ©minine franÃ§aise
    )

    await communicate.save(audio_file)

    return f"/audio/{text_hash}.mp3"

# Route pour servir les fichiers audio
@app.get("/audio/{filename}")
async def serve_audio(filename: str):
    file_path = f"data/tts_cache/{filename}"
    if os.path.exists(file_path):
        return FileResponse(file_path, media_type="audio/mpeg")
    raise HTTPException(404, "Audio file not found")
```

---

## ðŸ“¦ Structure du Projet Backend

```
nlp-module/backend-app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI app + routes
â”‚   â”œâ”€â”€ models.py               # Pydantic models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm.py             # Groq LLM calls
â”‚   â”‚   â”œâ”€â”€ tts.py             # Edge TTS
â”‚   â”‚   â””â”€â”€ session.py         # Session management
â”‚   â””â”€â”€ config.py              # Configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tts_cache/             # Fichiers audio MP3
â”œâ”€â”€ .env                       # GROQ_API_KEY
â”œâ”€â”€ requirements.txt
â””â”€â”€ run.py                     # Entry point
```

---

## ðŸ§ª Test du Backend

### Test 1 : Start Conversation

```bash
curl -X POST http://localhost:8000/conversation/start \
  -H "Content-Type: application/json" \
  -d '{
    "artwork_data": {
      "id": "1",
      "code": "AFR001",
      "title": "La Plaque en Bronze du BÃ©nin",
      "artist": "Artisans Edo",
      "description": "Une magnifique plaque...",
      "period": "16Ã¨me-17Ã¨me siÃ¨cle",
      "style": "Bronze du BÃ©nin",
      "collection": "Art d'\''Afrique de l'\''Ouest",
      "country": "Nigeria"
    }
  }'
```

**RÃ©ponse attendue:**
```json
{
  "session_id": "abc-123-def",
  "message": "Bienvenue ! Je suis Musia...",
  "audio_url": "/audio/hash123.mp3"
}
```

### Test 2 : Send Message

```bash
curl -X POST http://localhost:8000/conversation/text \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "abc-123-def",
    "message": "Qui a crÃ©Ã© cette Å“uvre ?",
    "artwork_data": {
      "id": "1",
      "title": "La Plaque en Bronze du BÃ©nin",
      "artist": "Artisans Edo",
      ...
    }
  }'
```

**RÃ©ponse attendue:**
```json
{
  "response": "Cette Å“uvre a Ã©tÃ© crÃ©Ã©e par les Artisans Edo...",
  "audio_url": "/audio/hash456.mp3",
  "intent": "general_question"
}
```

---

## âœ… Checklist Backend

- [ ] Installer dÃ©pendances : `groq`, `edge-tts`, `fastapi`, `uvicorn`
- [ ] CrÃ©er `.env` avec `GROQ_API_KEY`
- [ ] Modifier `/conversation/start` pour accepter `artwork_data`
- [ ] Modifier `/conversation/text` pour accepter `artwork_data`
- [ ] ImplÃ©menter `generate_welcome_message()`
- [ ] ImplÃ©menter `generate_ai_response()`
- [ ] ImplÃ©menter `generate_tts()`
- [ ] Tester avec `curl`
- [ ] Tester avec frontend

---

## ðŸŽ‰ RÃ©sultat Final

Une fois le backend modifiÃ© :

```
Frontend (JSON) â†’ Backend (LLM + TTS) â†’ Frontend (RÃ©ponse IA + Audio)
```

**Avantages :**
- âœ… Pas de base de donnÃ©es nÃ©cessaire
- âœ… 6 Å“uvres africaines contrÃ´lÃ©es (JSON)
- âœ… RÃ©ponses IA intelligentes (Groq)
- âœ… Audio professionnel (Edge TTS)
- âœ… Simple Ã  dÃ©ployer

---

**Le backend est maintenant compatible avec le mode JSON + IA !** ðŸš€
